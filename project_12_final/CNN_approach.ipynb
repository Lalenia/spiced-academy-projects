{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_approach.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMmusrg2gQ05"
      },
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "import io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import tensorflow as tf\n",
        "import torchaudio.transforms as T\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import math\n",
        "import tarfile\n",
        "import multiprocessing\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import scipy\n",
        "import librosa\n",
        "#import boto3\n",
        "#from botocore import UNSIGNED\n",
        "#from botocore.config import Config\n",
        "\n",
        "from IPython.display import Audio, display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIAsgfe24mmd",
        "outputId": "8a5f6040-4be1-471e-98f5-10fa3355af98"
      },
      "source": [
        "!pip3 install torchaudio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuFl9TEkF8Zx"
      },
      "source": [
        "This approach was based on this paper. Convolutional Neural Networks for Raw Speech Recognition\n",
        "Vishal Passricha and Rajesh Kumar Aggarwal http://dx.doi.org/10.5772/intechopen.80026\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thjhwBq-1j6-"
      },
      "source": [
        "We use torchaudio to download and represent the dataset. Here we use SpeechCommands, which is a datasets of 35 commands spoken by different people. The dataset SPEECHCOMMANDS is a torch.utils.data.Dataset version of the dataset. In this dataset, all audio files are about 1 second long (and so about 16000 time frames long)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFEdDHtxo4BT"
      },
      "source": [
        "This part taken but changed from  https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio.html\n",
        "\n",
        "A data point in the SPEECHCOMMANDS dataset is a tuple made of a waveform (the audio signal), the sample rate, the utterance (label), the ID of the speaker, the number of the utterance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JPtY_TngeCX"
      },
      "source": [
        "class DataSplitter(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(\"./\", download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            self.filepath = os.path.join(self._path, filename)\n",
        "            with open(self.filepath) as fileobj:\n",
        "                return [os.path.join(self._path, line.strip()) for line in fileobj]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQeiWHyG1icC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyJO5-h4hMYR",
        "outputId": "d0d29f14-b913-47ca-9d63-fa5258b07338"
      },
      "source": [
        "train_set = DataSplitter(\"training\")\n",
        "test_set = DataSplitter(\"testing\")\n",
        "val_set = DataSplitter(\"validation\")\n",
        "\n",
        "waveform, sample_rate, label, speaker_id, utterance_number = train_set[190]\n",
        "waveform.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1FamU9fd63M",
        "outputId": "0e6d877c-7861-4cfb-e6b4-82cee3fa745b"
      },
      "source": [
        "waveform, sample_rate, label, speaker_id, utterance_number"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.8311e-04, -3.3569e-04, -9.1553e-05,  ..., -1.2512e-03,\n",
              "          -3.0518e-04, -5.4932e-04]]), 16000, 'backward', '1e02ffc5', 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMFOTUfRsLOe"
      },
      "source": [
        "Here  several helper functions on how to deal the data for the Neaural Network. Not all will be used. A considerable EDA and FE on the data is to be seen on the eda_fe notebook. Here i decided to use the Transforms Classes from Pytorch\n",
        "as i've understood they are similal to the Column Transformers from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bX2_Z3zpb3r"
      },
      "source": [
        "def data_for_data_loader(data):\n",
        "  '''A function to return the data for the DataLoader class to iterate when training the model'''\n",
        "\n",
        "  for i in tqdm(range(64_747)):\n",
        "    waveform, sample_rate, label, speaker_id, utterance_number = data[i]#train_set[i]\n",
        "    #normalize the data\n",
        "    waveform = (waveform - waveform.mean()) / waveform.std()\n",
        "    mel_spectogram = create_melspectogram(waveform)\n",
        "    label = \n",
        "\n",
        "  return mel_spectogram, label\n",
        "\n",
        "\n",
        "def normalize_data(data):\n",
        "  waveform_list = []\n",
        "\n",
        "  for i in tqdm(range(64_747)):\n",
        "    waveform, sample_rate, label, speaker_id, utterance_number = train_set[i]\n",
        "    waveform = (waveform - waveform.mean()) / waveform.std()\n",
        "    waveform_list.append((waveform, sample_rate, label))\n",
        "\n",
        "  return waveform_list\n",
        "\n",
        "\n",
        "def one_hot_encoded_labels(data):\n",
        "  waveform_list = data_for_data_loader(data)\n",
        "\n",
        "  labels_ = sorted(list(set(datapoint[2] for datapoint in waveform_list)))\n",
        "  #if the labels are tensors!\n",
        "  labels_ = torch.nn.functional.one_hot(labels_)\n",
        "  return labels_\n",
        "\n",
        "#functions taken from:\n",
        "def label_to_index(word, data):\n",
        "  waveform_list = data_for_data_loader(data)\n",
        "  labels_ = sorted(list(set(datapoint[2] for datapoint in waveform_list)))\n",
        "   # Return the position of the word in labels\n",
        "  return torch.tensor(labels_.index(word))\n",
        "\n",
        "def encode_labels(data):\n",
        "    # A data tuple has the form:\n",
        "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
        "  waveform_list = data_for_data_loader(data)\n",
        "  labels_ = sorted(list(set(datapoint[2] for datapoint in waveform_list)))\n",
        "  targets = []\n",
        "  # Gather in lists, and encode labels as indices\n",
        "  for label in labels_:\n",
        "     targets += [label_to_index(label,data)]\n",
        "\n",
        "  targets = torch.stack(targets)\n",
        "\n",
        "  return targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6v05wCN6_kR"
      },
      "source": [
        "PyTorch works with DataLoader Classes -also with iterators and generators- that load the transformed data to the model. Here i have used in parts code and refactored it, changed it from different sources.\n",
        "When uploaded to my public repository the sourses will be mentioned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FXGJ056KaHe"
      },
      "source": [
        "approach on label encodin form here https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nFfH5xf7PKa",
        "outputId": "ce1ed203-70e4-40c5-d6a4-1ea29ed05cfc"
      },
      "source": [
        "#PyTorch Transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "class TextTransformer:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        a 2\n",
        "        b 3\n",
        "        c 4\n",
        "        d 5\n",
        "        e 6\n",
        "        f 7\n",
        "        g 8\n",
        "        h 9\n",
        "        i 10\n",
        "        j 11\n",
        "        k 12\n",
        "        l 13\n",
        "        m 14\n",
        "        n 15\n",
        "        o 16\n",
        "        p 17\n",
        "        q 18\n",
        "        r 19\n",
        "        s 20\n",
        "        t 21\n",
        "        u 22\n",
        "        v 23\n",
        "        w 24\n",
        "        x 25\n",
        "        y 26\n",
        "        z 27\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchaudio/functional/functional.py:433: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  \"At least one mel filterbank has all zero values. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WumddVD436Z"
      },
      "source": [
        "train_audio_transforms = nn.Sequential(\n",
        "    #here changes-> sample rate\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=1024, hop_length=512, n_mels=64)\n",
        "    \n",
        "#Even these transforms would improve acoording papers and documentation the performance of the model, i removed them after testing the model with these\n",
        "#and getting errors.\n",
        "    #torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    #torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        "    #Here i apply also MFFC\n",
        "    #torchaudio.transforms.MFFC(sample_rate= sample_rate,\n",
        "                # n_mfcc= 40,\n",
        "                 #dct_type = 2,\n",
        "                 #norm = 'ortho',\n",
        "                 #log_mels= True),\n",
        "    #torchaudio.transforms.AmplitudeToDB(stype: str = 'power', top_db: Optional[float] = None)            \n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "text_transform = TextTransform()\n",
        "\n",
        "#These functions will be in the DataHandler Class\n",
        "def transform_waveforms(data):\n",
        "  '''A function that converts waveforms to Mel Spectograms'''\n",
        "\n",
        "  for (waveform, sample_rate, label, speaker_id, utterance_number) in data:\n",
        "    #function squeeze  removes dimensions\n",
        "    spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "    waveform = (waveform - waveform.mean()) / waveform.std()\n",
        "    spectrograms = []\n",
        "    spectrograms.append(spec)\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "#here to apply a log() for normalizing the data-> Its already made by the Transform class\n",
        "  return spectrograms\n",
        "\n",
        "def transform_labels(data):\n",
        "  '''A function that converts the labels to tensors'''\n",
        "  labels = []\n",
        "\n",
        "  for (waveform, sample_rate, label, speaker_id, utterance_number) in data:\n",
        "    label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "    labels.append(label)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "  return labels\n",
        "\n",
        "def transform_data(data):\n",
        "  '''Returns melspectograms and lables as 4 dimensional tensors'''\n",
        "   spectrograms = []\n",
        "   labels = []\n",
        "\n",
        "   for (waveform, sample_rate, label, speaker_id, utterance_number) in data:\n",
        "     waveform = (waveform - waveform.mean()) / waveform.std()\n",
        "     spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "     spectrograms_.append(spec)\n",
        "     label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "     labels.append(label)\n",
        "     #as it seems the labels should be also tensors\n",
        "     #This function here pads a list of variable length Tensors with padding_value pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length.\n",
        "     spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "     labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "   return spectrograms, labels\n",
        "\n",
        "def transform_waveform(waveform):\n",
        "  '''Returns a melspectogram as a 4 dimensional tensor'''  \n",
        "  train_audio_transforms = nn.Sequential(\n",
        "  torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=1024,\n",
        "        hop_length=512,\n",
        "        n_mels=64)       \n",
        "  )\n",
        "  valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "  text_transform = TextTransform()\n",
        "  waveform = (waveform - waveform.mean()) / waveform.std()\n",
        "  melspectogram = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "  return melspectogram\n",
        "\n",
        "\n",
        "def waveform_to_melspectogram(data):\n",
        "  '''Returns a melspectogram as a 2 dimensional tensor'''\n",
        "  n_fft = 1024\n",
        "  win_length = None\n",
        "  hop_length = 512\n",
        "  n_mels = 128\n",
        "  for i in tqdm(range(64_747)):\n",
        "    waveform, sample_rate, label, speaker_id, utterance_number = train_set[i]\n",
        "    mel_spectrogram = T.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, win_length=win_length, hop_length=hop_length, center=True,pad_mode=\"reflect\",\n",
        "                                       power=2.0, norm='slaney', onesided=True, n_mels=n_mels, mel_scale=\"htk\",\n",
        ")\n",
        "    waveform = (waveform - waveform.mean()) / waveform.std()\n",
        "    mel_spectogram = mel_spectrogram(waveform)\n",
        "\n",
        "    return mel_spectogram\n",
        "\n",
        "def create_melspectogram(waveform):\n",
        "  n_fft = 1024\n",
        "  win_length = None\n",
        "  hop_length = 512\n",
        "  n_mels = 128\n",
        "\n",
        "  mel_spectrogram = T.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, win_length=win_length, hop_length=hop_length, center=True,pad_mode=\"reflect\",\n",
        "                                     power=2.0, norm='slaney', onesided=True, n_mels=n_mels, mel_scale=\"htk\")\n",
        "  waveform = (waveform - waveform.mean()) / waveform.std()\n",
        "  mel_spectogram = mel_spectrogram(waveform)\n",
        "\n",
        "  return mel_spectogram\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pciKT_gi9GVB",
        "outputId": "25f8be01-2fbb-464b-ab26-0cf9a3abf42a"
      },
      "source": [
        "waveform_to_melspectogram(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64747 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.5496e+01, 1.9524e+02, 4.2929e+02,  ..., 8.7284e+00,\n",
              "          1.7248e+02, 4.1024e+02],\n",
              "         [3.8688e+00, 9.7259e+01, 1.5132e+02,  ..., 2.7743e+01,\n",
              "          1.1723e+02, 8.7304e+01],\n",
              "         [3.0970e+00, 2.0773e+01, 2.6702e+01,  ..., 5.0211e+01,\n",
              "          4.3092e+01, 3.8962e+01],\n",
              "         ...,\n",
              "         [2.7885e-03, 1.1034e-03, 1.2200e-03,  ..., 7.5741e-04,\n",
              "          1.4870e-03, 3.9908e-03],\n",
              "         [7.2344e-04, 1.2109e-03, 9.4925e-04,  ..., 8.0535e-04,\n",
              "          5.9425e-04, 1.2161e-03],\n",
              "         [4.0984e-04, 3.2984e-04, 6.9453e-04,  ..., 6.2190e-04,\n",
              "          8.3409e-04, 3.2910e-04]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9TJPyzb4uU3",
        "outputId": "e2bb153a-bb1e-44c9-e661-1770f77045a2"
      },
      "source": [
        "melspectogram = transform_waveform(waveform)\n",
        "melspectogram"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchaudio/functional/functional.py:433: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  \"At least one mel filterbank has all zero values. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.4177e-04, 7.1539e-05, 1.2141e-04,  ..., 2.8229e-04, 4.1490e-04,\n",
              "         1.4119e-04],\n",
              "        [3.9075e-05, 2.3011e-04, 5.2196e-04,  ..., 4.0095e-04, 3.6825e-04,\n",
              "         1.3970e-04],\n",
              "        [1.7551e-04, 7.7636e-05, 2.7296e-04,  ..., 2.5858e-04, 1.5001e-04,\n",
              "         8.9354e-05],\n",
              "        ...,\n",
              "        [2.8391e-04, 7.9397e-04, 2.1794e-03,  ..., 8.4985e-04, 4.0586e-04,\n",
              "         3.9696e-04],\n",
              "        [5.8698e-04, 1.1963e-03, 6.0536e-04,  ..., 8.9311e-04, 1.0738e-03,\n",
              "         2.9306e-04],\n",
              "        [2.5178e-04, 9.2665e-04, 5.1814e-04,  ..., 5.5713e-04, 5.8877e-04,\n",
              "         1.9919e-04]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0kES9Yo-s0c",
        "outputId": "0ea8de8d-68cc-4a5a-e449-3d8b27fb57a8"
      },
      "source": [
        "mel_spectograms = transform_waveforms(train_set)\n",
        "mel_spectograms.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 64, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJHJ1gkJikZA"
      },
      "source": [
        "I dont want a list of mel spectograms i want a spec pro time!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9MDTKUeu2lu"
      },
      "source": [
        "# Different ways on define the layers at Pytorch.\n",
        "\n",
        "One way is this: the Sequeantial class which is a Container. It can be used also with the Transforms.The activation function is set here.\n",
        "\n",
        "The other seperates the steps. The activation function is activated at the forward function called on a Functional \"object\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nBqlIgcuifj"
      },
      "source": [
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "class CNNNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 4 conv blocks / flatten / linear / softmax\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(128 * 5 * 4, 10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        x = self.conv1(input_data)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear(x)\n",
        "        predictions = self.softmax(logits)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cnn = CNNNetwork()\n",
        "    summary(cnn.cuda(), (1, 64, 44))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrZ_Mi4Zrzpb"
      },
      "source": [
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CNNNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2)\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(128 * 5 * 4, 10)\n",
        "        #self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        x = self.conv1(input_data)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        #predictions = self.softmax(x)\n",
        "        predictions = F.log_softmax(x)# here i can have a problems with the dimensions if so it can be given in the parameters ex dim=2\n",
        "        return predictions\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cnn = CNNNetwork()\n",
        "    summary(cnn.cpu(), (1, 64, 44))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqjEXwK4NVVg"
      },
      "source": [
        "Preparing your data for training with DataLoaders\n",
        "\n",
        "The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
        "\n",
        "DataLoader is an iterable that abstracts this complexity for us in an easy API.\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCQtODnmrKLP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "c27e495a-23da-4855-b95d-8d11f79243f1"
      },
      "source": [
        "#My appproach\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Here i can use the RayTune hyperparameters.\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "def create_data_loader(train_data, batch_size):\n",
        "    train_dataloader = DataLoader(train_data, batch_size)#train_data have a def __len__(self)!\n",
        "    #here make a the data to be only melspectograms and the labels (in which form though????????)#As tensors or one hot encoded????\n",
        "    return train_dataloader\n",
        "\n",
        "#In Pytorch you write function for the train and test\n",
        "#the batch size is here  in data_loader. My input data are the melspectograms and labels\n",
        "def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n",
        "\n",
        "  for batch_idx, data in enumerate(data_loader):\n",
        "    #data here are the are the train_data = DataSplitter(\"training\"), which the train_set that use in the data_for_data_loader fumnction to take the\n",
        "    #mel_spectograms and labels. I don't use the function that returns a list because it would not iterate correct.\n",
        "    #input , target = data_for_data_loader(data)#2 dim tensors...\n",
        "    input , target = transform_data(data)\n",
        "    input, target = input.to(device), target.to(device)\n",
        "\n",
        "    # calculate loss\n",
        "    #To the model i give the melspectogram and it expects 4-dimensional input for 4-dimensional weight [16, 1, 3, 3]\n",
        "    prediction = model(input)\n",
        "    loss = loss_fn(prediction, target)\n",
        "\n",
        "        # backpropagate error and update weights\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimiser, device, epochs, BATCH_SIZE = 128):\n",
        "    for i in range(epochs):\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "        print(\"---------------------------\")\n",
        "    print(\"Finished training\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "#device in Pytorch is a specification on whether a GPU or CPU is used from the machine's user. If there is a GPU available then there is a CUDA selection.\n",
        "#f.y.i. Macintosh with OS higher than HighSierra can't use the CUDA selection!\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cpu\"\n",
        "  print(f\"Using {device}\")\n",
        "\n",
        "    train_data = DataSplitter(\"training\")\n",
        "    test_data = DataSplitter(\"testing\")\n",
        "    val_data = DataSplitter(\"validation\")\n",
        "    \n",
        "    train_dataloader = create_data_loader(train_set, BATCH_SIZE)\n",
        "\n",
        "    # construct model and assign it to device\n",
        "    cnn = CNNNetwork().to(device)\n",
        "    print(cnn)\n",
        "\n",
        "    # initialise loss funtion + optimiser\n",
        "    loss_fn = nn.CTCLoss(blank=28).to(device)\n",
        "    optimiser = torch.optim.Adam(cnn.parameters(),\n",
        "                                 lr=LEARNING_RATE)\n",
        "\n",
        "    # train model\n",
        "    train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n",
        "\n",
        "    #Try on one\n",
        "    #train(cnn, melspectogram, loss_fn, optimiser, device, EPOCHS,BATCH_SIZE = 128)\n",
        "    \n",
        "\n",
        "    # save model\n",
        "    torch.save(cnn.state_dict(), \"feedforwardnet.pth\")\n",
        "    print(\"Trained feed forward net saved at feedforwardnet.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu\n",
            "CNNNetwork(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear): Linear(in_features=2560, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-5c2d395081f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m#Try on one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-5c2d395081f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, loss_fn, optimiser, device, epochs, BATCH_SIZE)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtrain_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-5c2d395081f0>\u001b[0m in \u001b[0;36mtrain_single_epoch\u001b[0;34m(model, data_loader, loss_fn, optimiser, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m#data here are the are the train_data = DataSplitter(\"training\"), which the train_set that use in the data_for_data_loader fumnction to take the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#mel_spectograms and labels. I don't use the function that returns a list because it would not iterate correct.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 16000] at entry 0 and [1, 10922] at entry 47"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FkcQTadjlXM"
      },
      "source": [
        "#To turn the int back to text so as to be able to read the prediction i used the Greedy Encoder. Much of this part are taken from DeepSpeech2 approach on Speech Recognition \n",
        "#models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_s8sKq1kIkw"
      },
      "source": [
        "#Test the model\n",
        "\n",
        "A future approach on how to encode and decode the target\n",
        "variable. The markov hypothesis based in Markov chains based on probabilities for each letter of the word. The CTC loss function i used above is a part of this  approach.\n",
        "\n",
        "Here i took another approach, in which the model takes as an input a wav file that will be be wrangled the same way as the train, test and validation data and will map the word to a given dict with correct words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWCWXy3SkJIB"
      },
      "source": [
        "class_mapping = ['backward',\n",
        " 'bed',\n",
        " 'bird',\n",
        " 'cat',\n",
        " 'dog',\n",
        " 'down',\n",
        " 'eight',\n",
        " 'five',\n",
        " 'follow',\n",
        " 'forward',\n",
        " 'four',\n",
        " 'go',\n",
        " 'happy',\n",
        " 'house',\n",
        " 'learn',\n",
        " 'left',\n",
        " 'marvin',\n",
        " 'nine',\n",
        " 'no',\n",
        " 'off',\n",
        " 'on',\n",
        " 'one',\n",
        " 'right',\n",
        " 'seven',\n",
        " 'sheila',\n",
        " 'six',\n",
        " 'stop',\n",
        " 'three',\n",
        " 'tree',\n",
        " 'two',\n",
        " 'up',\n",
        " 'visual',\n",
        " 'wow',\n",
        " 'yes',\n",
        " 'zero']\n",
        "\n",
        "\n",
        "def predict(model, input, target, class_mapping):\n",
        "    model.eval()\n",
        "    #\n",
        "    with torch.no_grad():\n",
        "      #This function here :Context-manager that disabled gradient calculation is disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). \n",
        "      #It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
        "\n",
        "        predictions = model(input)\n",
        "        predicted_index = predictions[0].argmax(0)\n",
        "        predicted = class_mapping[predicted_index]\n",
        "        expected = class_mapping[target]\n",
        "    return predicted, expected\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # load back the model\n",
        "    cnn = CNNNetwork()\n",
        "    state_dict = torch.load(\"cnnnet.pth\")\n",
        "    cnn.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "    # get a sample from the SpeechCommands Dataset\n",
        "    input, target = test_set[0][0], test_set[0][1] # [batch size, waveform, sample_rate etc]\n",
        "    input.unsqueeze_(0)\n",
        "\n",
        "    # make an inference\n",
        "    predicted, expected = predict(cnn, input, target,\n",
        "                                  class_mapping)\n",
        "    print(f\"Predicted: '{predicted}', expected: '{expected}'\")\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8bWdfchtjOx"
      },
      "source": [
        "A"
      ]
    }
  ]
}